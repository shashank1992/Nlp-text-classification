{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6803156,"sourceType":"datasetVersion","datasetId":3914430},{"sourceId":6816604,"sourceType":"datasetVersion","datasetId":3920837},{"sourceId":6817108,"sourceType":"datasetVersion","datasetId":3921089},{"sourceId":6847272,"sourceType":"datasetVersion","datasetId":3936398}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-17T18:12:53.390424Z","iopub.execute_input":"2023-11-17T18:12:53.391272Z","iopub.status.idle":"2023-11-17T18:12:53.705099Z","shell.execute_reply.started":"2023-11-17T18:12:53.391239Z","shell.execute_reply":"2023-11-17T18:12:53.704127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!!sudo apt-get install swig3.0","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:21:59.029945Z","iopub.execute_input":"2023-11-17T15:21:59.030376Z","iopub.status.idle":"2023-11-17T15:22:04.273159Z","shell.execute_reply.started":"2023-11-17T15:21:59.030342Z","shell.execute_reply":"2023-11-17T15:22:04.272221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nltk\n!pip install seaborn\n!pip install datasets transformers\n!pip install tensorflow_text\nfrom keras.utils import pad_sequences\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score ,confusion_matrix\nimport pandas as pd\nimport numpy as np\nimport transformers\nfrom transformers import BertTokenizer\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom keras.utils import to_categorical\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nfrom gensim.utils import simple_preprocess\nimport tensorflow_text as text\nimport re\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import SpatialDropout1D, Conv1D, MaxPooling1D, LSTM","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:13:03.332503Z","iopub.execute_input":"2023-11-17T18:13:03.333397Z","iopub.status.idle":"2023-11-17T18:13:50.498261Z","shell.execute_reply.started":"2023-11-17T18:13:03.333363Z","shell.execute_reply":"2023-11-17T18:13:50.497153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Loading the Data****","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/train-dataset-csv/train_dataset.csv\"\ntest_path = \"/kaggle/input/test-data-set-csv/test_dataset.csv\"\nsample_submission = pd.read_csv(\"/kaggle/input/sample-submission/sample_submission.csv\")\ntrain_data = pd.read_csv(dataset_path)\ntest_data = pd.read_csv(test_path)\ntrain_data.drop(columns=['qid'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:15:24.426479Z","iopub.execute_input":"2023-11-17T18:15:24.427344Z","iopub.status.idle":"2023-11-17T18:15:28.935707Z","shell.execute_reply.started":"2023-11-17T18:15:24.427310Z","shell.execute_reply":"2023-11-17T18:15:28.934879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:15:32.611825Z","iopub.execute_input":"2023-11-17T18:15:32.612239Z","iopub.status.idle":"2023-11-17T18:15:32.629530Z","shell.execute_reply.started":"2023-11-17T18:15:32.612207Z","shell.execute_reply":"2023-11-17T18:15:32.628246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"markdown","source":"Remove stopwords and preprocessing.","metadata":{}},{"cell_type":"code","source":"def  clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"'ll\", \" will\", text)\n    text = re.sub(r\"'ve\", \" have\", text)\n    text = re.sub(r\"'re\", \" are\", text)\n    text = re.sub(r\"'d\", \" would\", text)\n    text = re.sub(r\"'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()#/@;:{}`+=~|.!?,]\", \"\", text)\n    text = text.translate(str.maketrans('', '', string.punctuation)) \n    text = re.sub(\"(\\W)\",\" \",text) \n    return text\n\ndef adjust(sen):\n    return trie.getWord(sen)\n\ndef preprocess(train_data):\n    train_data['question_text'] = train_data['question_text'].apply(lambda x: clean_text(x))\n    train_data['question_text'] = train_data['question_text'].apply(lambda x: adjust(x))\n    train_data['question_text'] = train_data['question_text'].apply(lambda x: simple_preprocess(x,max_len=30))\n    train_data['question_text'] = train_data['question_text'].apply(lambda x: [w for w in x if w not in english_stopwords])","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:15:38.524151Z","iopub.execute_input":"2023-11-17T18:15:38.524506Z","iopub.status.idle":"2023-11-17T18:15:38.535530Z","shell.execute_reply.started":"2023-11-17T18:15:38.524478Z","shell.execute_reply":"2023-11-17T18:15:38.534674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using pretrained Glove vectors to build the dictionary of words to vectors**","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nvocabulary = []\nf= open('/kaggle/input/glove-200d/glove.6B.200d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    vocabulary.append(word)\n    coeffs = np.asarray(values[1:],dtype='float16')\n    embeddings_index[word] = coeffs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building a trie to handle out of vocabulary problem**","metadata":{}},{"cell_type":"code","source":"class Trie:\n    def __init__(self,char,children,isend=False):\n        self.char = char\n        self.children = children\n        self.isend = isend\n    \n    def insert(self, word):\n        node = self\n        for char in word:\n            node = node.children.setdefault(char,Trie(char,{}))\n        node.isend = True\n    \n      \n    \n    def isSeqPresent(self,word):\n        node = self\n        for char in word:\n            node = node.children.get(char,None)\n            if not node:\n                return 0\n        return 1\n    \n    def getWord(self, word, n=2):\n        node = self\n        result = \"\"\n        letter_count = 0\n        sug = []\n        ans=''\n        for char in word:\n            if char == ' ':\n                node = self\n                ans += result + ' '\n                result = \"\"\n                letter_count = 0\n                sug = []\n                continue\n            if not node.children.get(char,None):\n                if len(sug) > 0:\n                    val = sug.pop()\n            # carry the excess chars from previous word\n                    result = result.replace(val,'')\n                    ans += val+' '\n                else: result = ''\n                node = self\n                letter_count =len(result)\n            if node.children.get(char,None):\n                node = node.children[char]\n                result +=char\n                letter_count +=1\n    #          for two letter words and above  maintain a stack\n                if letter_count>1 and node.isend:\n                    sug.append(result)\n        ans += result\n        return ans\n\n# class Info:\n#     def __init__(self, result='',ans='',sug):\n#         self.result = result\n#         self.ans = ans\n#         self.sug = sug\n        \ndef get_word_2(self, word, n=2):\n        node = self\n        result = \"\"\n        letter_count = 0\n        sug = []\n        ans=''\n        for char in word:\n            if char == ' ':\n                node = self\n                ans += result + ' '\n                result = \"\"\n                letter_count = 0\n                sug = []\n                continue\n            if not node.children.get(char,None):\n                if len(sug) > 0:\n                    val = sug.pop()\n                    sug = []\n            # carry the excess chars from previous word\n                    result = result.replace(val,'')\n                    ans += val+' '\n                else: result = ''\n                node = self\n                for i in result:\n                    if node.children.get(i,None):\n                        node = node.children[i]\n                    else: \n                        node = self\n                        result = ''\n                letter_count =len(result)\n            if node.children.get(char,None):\n                node = node.children[char]\n                result +=char\n                letter_count +=1\n        #          for two letter words and above  maintain a stack\n                if letter_count>1 and node.isend:\n                    sug.append(result)\n        ans += result\n        return ans\n\nprint('initialising Trie')\n\ntrie = Trie('dummy',{})\ndef build_trie(vocabulary_trim):\n    for word in vocabulary_trim:\n        trie.insert(word)\n        \ntrie.getWord = get_word_2.__get__(trie)\nprint('initialising vocabulary')\nvocabulary_trim = [x for x in vocabulary if len(x)>1]\nprint('The length of trimmed vocabulary',len(vocabulary_trim))\nprint('building trie')\nbuild_trie(vocabulary_trim)\nprint('successfully built the trie')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Testing the trie**","metadata":{}},{"cell_type":"code","source":"# sample words that trie effectively handles\n\ntrie.getWord('exboyfriend')\ntrie.isPresent('demonetize')\ntrie.getWord('bestfriend, nonmuslim, dropshipping quorans pakistangs womangs persongs himher exgirlfriend altright')","metadata":{"execution":{"iopub.status.busy":"2023-11-03T12:12:22.071447Z","iopub.execute_input":"2023-11-03T12:12:22.071964Z","iopub.status.idle":"2023-11-03T12:12:22.080376Z","shell.execute_reply.started":"2023-11-03T12:12:22.071923Z","shell.execute_reply":"2023-11-03T12:12:22.079276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing ","metadata":{}},{"cell_type":"code","source":"preprocess(train_data)\npreprocess(test_data)\ntest_data.head()\n\ny=train_data['target'].values\nprint(y[:5])\n\n# Compute class weights\nclass_weights = compute_class_weight('balanced', classes=[0, 1], y=y)\nclass_weights = dict(enumerate(class_weights))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:19:26.980096Z","iopub.execute_input":"2023-11-17T18:19:26.980783Z","iopub.status.idle":"2023-11-17T18:21:34.585239Z","shell.execute_reply.started":"2023-11-17T18:19:26.980751Z","shell.execute_reply":"2023-11-17T18:21:34.584429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def target_ratio(y):\n    count = 0\n    for val in y:\n        if val==1: count+=1\n    print('number 1s are %s and number of 0s are %s and ratio is %s'%(count, len(y)-count, count/len(y)))\n\n# verifying the class weights\nprint('class weights', class_weights)\ntarget_ratio(y)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing the text","metadata":{}},{"cell_type":"code","source":"MAX_LEN=30\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data['question_text'])\nX=tokenizer.texts_to_sequences([' '.join(seq[:MAX_LEN]) for seq in train_data['question_text']])\nX_test_data=tokenizer.texts_to_sequences([' '.join(seq[:MAX_LEN]) for seq in test_data['question_text']])\n\n# padding the train data\nX = pad_sequences(X, maxlen=MAX_LEN, padding='post', truncating='post')\n\n# padding the test data\nX_test_data = pad_sequences(X_test_data, maxlen=MAX_LEN, padding='post', truncating='post')\n\n# splitting the train data into train and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:21:59.567463Z","iopub.execute_input":"2023-11-17T18:21:59.568654Z","iopub.status.idle":"2023-11-17T18:21:59.572641Z","shell.execute_reply.started":"2023-11-17T18:21:59.568596Z","shell.execute_reply":"2023-11-17T18:21:59.571700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the embedding matrix","metadata":{}},{"cell_type":"code","source":"words_not_found = []\nvocab_size = len(tokenizer.word_index)+1\nembedding_dim = 200\nembedding_matrix = np.zeros((vocab_size,embedding_dim))\nfor word,i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None and len(embedding_vector) > 0:\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:23:26.780902Z","iopub.execute_input":"2023-11-17T18:23:26.781709Z","iopub.status.idle":"2023-11-17T18:23:27.205732Z","shell.execute_reply.started":"2023-11-17T18:23:26.781675Z","shell.execute_reply":"2023-11-17T18:23:27.204911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identifying the OOV**","metadata":{}},{"cell_type":"code","source":"test_vocab = words_not_found[:100]\ntest_vocab = ' '.join(test_vocab)\ntest_vocab\nwords_cs = pd.DataFrame(words_not_found)\nwords_cs.to_csv('words_not_found.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:23:32.950656Z","iopub.execute_input":"2023-11-17T18:23:32.951505Z","iopub.status.idle":"2023-11-17T18:23:32.955449Z","shell.execute_reply.started":"2023-11-17T18:23:32.951476Z","shell.execute_reply":"2023-11-17T18:23:32.954452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nembedding_layer = Embedding(vocab_size,\n                            embedding_dim,\n                            weights = [embedding_matrix],\n                            input_length = MAX_LEN,\n                            trainable=False)\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.50, name='first_gru_layer')))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(GRU(64, name='second_gru_layer')))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid', name='output_layer'))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:25:06.983068Z","iopub.execute_input":"2023-11-17T18:25:06.983959Z","iopub.status.idle":"2023-11-17T18:25:11.119301Z","shell.execute_reply.started":"2023-11-17T18:25:06.983921Z","shell.execute_reply":"2023-11-17T18:25:11.118315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:25:17.209296Z","iopub.execute_input":"2023-11-17T18:25:17.210018Z","iopub.status.idle":"2023-11-17T18:25:17.228281Z","shell.execute_reply.started":"2023-11-17T18:25:17.209979Z","shell.execute_reply":"2023-11-17T18:25:17.227273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:25:19.860413Z","iopub.execute_input":"2023-11-17T18:25:19.861331Z","iopub.status.idle":"2023-11-17T18:25:19.892251Z","shell.execute_reply.started":"2023-11-17T18:25:19.861296Z","shell.execute_reply":"2023-11-17T18:25:19.891389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\nN_EPOCHS = 8","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:25:32.427210Z","iopub.execute_input":"2023-11-17T18:25:32.427917Z","iopub.status.idle":"2023-11-17T18:25:32.432579Z","shell.execute_reply.started":"2023-11-17T18:25:32.427874Z","shell.execute_reply":"2023-11-17T18:25:32.431411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train,\n          batch_size=BATCH_SIZE,\n          epochs=N_EPOCHS,\n          validation_data=(X_test, y_test),\n          class_weight=class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:25:52.951433Z","iopub.execute_input":"2023-11-17T18:25:52.952189Z","iopub.status.idle":"2023-11-17T18:37:37.412529Z","shell.execute_reply.started":"2023-11-17T18:25:52.952152Z","shell.execute_reply":"2023-11-17T18:37:37.411663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting with the model","metadata":{}},{"cell_type":"code","source":"# obtaining accuracy with the validation data\ny_s = model.predict(X_test)\ny_s_r = [1 if val > 0.83 else 0 for val in y_s]\ny_s_r[:2]\n\n\n\ncr = pd.DataFrame(classification_report(y_test,y_s_r,output_dict=True)).T\ncr['support'] = cr.support.apply(int)\ncr.style.background_gradient(cmap='Pastel1')","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:38:38.998768Z","iopub.execute_input":"2023-11-17T18:38:38.999717Z","iopub.status.idle":"2023-11-17T18:39:11.505330Z","shell.execute_reply.started":"2023-11-17T18:38:38.999681Z","shell.execute_reply":"2023-11-17T18:39:11.504325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the predictions on the test data\ny_test_data = model.predict(X_test_data)\ny_test_data1[:2]\ny_test_data1 = [1 if val > 0.8 else 0 for val in y_test_data]\ntest_data['target'] = y_test_data1\ntest_data['target'].value_counts()\ntest_data.drop(columns=[\"question_text\"],inplace=True)\ntest_data['qid'] = sample_submission['qid']\ntest_data.to_csv(\"test_results.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T06:24:06.868426Z","iopub.execute_input":"2023-11-03T06:24:06.868880Z","iopub.status.idle":"2023-11-03T06:27:48.549775Z","shell.execute_reply.started":"2023-11-03T06:24:06.868844Z","shell.execute_reply":"2023-11-03T06:27:48.548301Z"},"trusted":true},"execution_count":null,"outputs":[]}]}